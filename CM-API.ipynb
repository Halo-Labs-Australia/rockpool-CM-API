{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Must be set before SparkContext / SparkSession is created!\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467e893c-2f70-4cb0-ade0-19fa6ddf2109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "# from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CM\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977e3aa5-02bf-48cc-b5f1-4c42a0e893f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM token successfully retrieved\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import base64\n",
    "def get_cm_access_token():\n",
    "    # Credentials\n",
    "    password = \"\"\n",
    "    username = \"cmapi\"\n",
    "    # Token URL\n",
    "    token_url = \"https://rockpoolrac.residential.icarehealth.com.au/Tetra.Web/OAuth/Token\"\n",
    "    # Encode Basic Auth header\n",
    "    credentials = f\"{username}:{password}\"\n",
    "    encoded_credentials = base64.b64encode(credentials.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "    # Headers and form data\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Basic {encoded_credentials}\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "    data = {\n",
    "        \"grant_type\": \"client_credentials\"\n",
    "    }\n",
    "    # Token request\n",
    "    response = requests.post(token_url, headers=headers, data=data)\n",
    "    if response.status_code == 200:\n",
    "        token = response.json().get(\"access_token\")\n",
    "        return token\n",
    "    else:\n",
    "        raise Exception(f\"Failed to retrieve token: {response.status_code} - {response.text}\")\n",
    "# Test token retrieval\n",
    "cm_access_token = get_cm_access_token()\n",
    "if cm_access_token:\n",
    "    print(\"CM token successfully retrieved\")\n",
    "else:\n",
    "    print(\"Token retrieval failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://rockpoolrac.residential.icarehealth.com.au/Tetra.Web\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e4ab7f-e46c-4b72-8d1d-c4490c8f6735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {cm_access_token}\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "url = f\"{BASE_URL}/api/ExternalAssessmentForm/GetAssessmentFormResponses?formId=152890\"\n",
    "resp = requests.get(url, headers=headers)\n",
    "resp.raise_for_status()\n",
    "json_text = resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = resp.json()\n",
    "if isinstance(record, dict) and \"data\" in record:\n",
    "    records = record[\"data\"]\n",
    "\n",
    "\n",
    "# # Quick sanity check:\n",
    "# for i, rec in enumerate(records[:5]):\n",
    "#     print(i, type(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Let Spark infer the schema from your JSON text\n",
    "#    • We create a one‑row DF with a F.column \"json\", then ask for its schema.\n",
    "schema = (\n",
    "    spark\n",
    "      .range(1)                                  # dummy DF with one row\n",
    "      .select(F.schema_of_json(F.lit(json_text)).alias(\"inferred\"))\n",
    "      .collect()[0][\"inferred\"]                 # pull out the StructType\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Create a DataFrame holding your JSON string\n",
    "raw = spark.createDataFrame([(json_text,)], [\"json\"])\n",
    "\n",
    "# 5. Parse it using the inferred schema\n",
    "df = raw.select(F.from_json(F.col(\"json\"), schema).alias(\"data\")).select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = (\n",
    "    df\n",
    "    # explode the top-level `items` array\n",
    "    .select(\n",
    "        F.col(\"formId\"),\n",
    "        F.explode(F.col(\"items\")).alias(\"item\")\n",
    "    )\n",
    "    # explode each item's `responses` array\n",
    "    .select(\n",
    "        \"formId\",\n",
    "        F.col(\"item.templateQuestion.text\").alias(\"question_text\"),\n",
    "        F.explode(F.col(\"item.responses\")).alias(\"resp\")\n",
    "    )\n",
    "    # pull the response fields up\n",
    "    .select(\n",
    "        \"formId\",\n",
    "        \"question_text\",\n",
    "        F.col(\"resp.stringValue\").alias(\"string_value\"),\n",
    "        F.col(\"resp.numericValue\").alias(\"numeric_value\"),\n",
    "        F.col(\"resp.booleanValue\").alias(\"boolean_value\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- formId: long (nullable = true)\n",
      " |-- question_text: string (nullable = true)\n",
      " |-- string_value: string (nullable = true)\n",
      " |-- numeric_value: double (nullable = true)\n",
      " |-- boolean_value: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o299.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 26) (HL-WIN-018.modem executor driver): java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:218)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 44 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:218)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 44 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Show your fully‑flattened table\u001b[39;00m\n\u001b[32m      2\u001b[39m flattened.printSchema()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m flattened.show(truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HarryZhan\\miniconda3\\envs\\rockpool\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m._show_string(n, truncate, vertical))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HarryZhan\\miniconda3\\envs\\rockpool\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:316\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    309\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    310\u001b[39m         messageParameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m         },\n\u001b[32m    314\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jdf.showString(n, int_truncate, vertical)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HarryZhan\\miniconda3\\envs\\rockpool\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HarryZhan\\miniconda3\\envs\\rockpool\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(*a, **kw)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HarryZhan\\miniconda3\\envs\\rockpool\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o299.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 26) (HL-WIN-018.modem executor driver): java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:218)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 44 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:218)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 44 more\r\n"
     ]
    }
   ],
   "source": [
    "# Show your fully‑flattened table\n",
    "flattened.printSchema()\n",
    "flattened.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "resp_questionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "resp_responseId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "resp_dateValue",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "resp_stringValue",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "resp_numericValue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "resp_booleanValue",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "resp_score",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "resp_recordInformation.id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "resp_recordInformation.userId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "resp_recordInformation.userNameFirst",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "resp_recordInformation.userNameLast",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "resp_recordInformation.userQualification",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "resp_recordInformation.agencyStaffName",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "resp_recordInformation.agencyStaffDesignation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "resp_recordInformation.dateCreated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "meta_formId",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "meta_formVersion",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "meta_status",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "meta_dateCompleted",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "meta_items.templateQuestion.text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2dbb4d6b-4b17-400b-b23d-290e0f21418e",
       "rows": [
        [
         "0",
         "123",
         null,
         null,
         null,
         null,
         "True",
         null,
         "4121620",
         "2821",
         "Shuvekshya",
         "Phuyal",
         "Registered Nurse",
         null,
         null,
         "2025-07-15T01:34:18.11Z",
         "152890",
         "1",
         "1",
         "2025-07-15T01:34:18Z",
         "PART 1 - FALLS RISK STATUS"
        ],
        [
         "1",
         "124",
         "30965.0",
         null,
         null,
         null,
         null,
         "8",
         "4121621",
         "2821",
         "Shuvekshya",
         "Phuyal",
         "Registered Nurse",
         null,
         null,
         "2025-07-15T01:34:18.11Z",
         "152890",
         "1",
         "1",
         "2025-07-15T01:34:18Z",
         "Risk Factor: recent falls"
        ],
        [
         "2",
         "125",
         "30968.0",
         null,
         null,
         null,
         null,
         "3",
         "4121622",
         "2821",
         "Shuvekshya",
         "Phuyal",
         "Registered Nurse",
         null,
         null,
         "2025-07-15T01:34:18.11Z",
         "152890",
         "1",
         "1",
         "2025-07-15T01:34:18Z",
         "Risk factor: medications"
        ],
        [
         "3",
         "126",
         "30972.0",
         null,
         null,
         null,
         null,
         "3",
         "4121623",
         "2821",
         "Shuvekshya",
         "Phuyal",
         "Registered Nurse",
         null,
         null,
         "2025-07-15T01:34:18.11Z",
         "152890",
         "1",
         "1",
         "2025-07-15T01:34:18Z",
         "Risk factor: psychological"
        ],
        [
         "4",
         "127",
         "30976.0",
         null,
         null,
         null,
         null,
         "3",
         "4121624",
         "2821",
         "Shuvekshya",
         "Phuyal",
         "Registered Nurse",
         null,
         null,
         "2025-07-15T01:34:18.11Z",
         "152890",
         "1",
         "1",
         "2025-07-15T01:34:18Z",
         "Risk factor: cognitive status"
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resp_questionId</th>\n",
       "      <th>resp_responseId</th>\n",
       "      <th>resp_dateValue</th>\n",
       "      <th>resp_stringValue</th>\n",
       "      <th>resp_numericValue</th>\n",
       "      <th>resp_booleanValue</th>\n",
       "      <th>resp_score</th>\n",
       "      <th>resp_recordInformation.id</th>\n",
       "      <th>resp_recordInformation.userId</th>\n",
       "      <th>resp_recordInformation.userNameFirst</th>\n",
       "      <th>resp_recordInformation.userNameLast</th>\n",
       "      <th>resp_recordInformation.userQualification</th>\n",
       "      <th>resp_recordInformation.agencyStaffName</th>\n",
       "      <th>resp_recordInformation.agencyStaffDesignation</th>\n",
       "      <th>resp_recordInformation.dateCreated</th>\n",
       "      <th>meta_formId</th>\n",
       "      <th>meta_formVersion</th>\n",
       "      <th>meta_status</th>\n",
       "      <th>meta_dateCompleted</th>\n",
       "      <th>meta_items.templateQuestion.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>4121620</td>\n",
       "      <td>2821</td>\n",
       "      <td>Shuvekshya</td>\n",
       "      <td>Phuyal</td>\n",
       "      <td>Registered Nurse</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-07-15T01:34:18.11Z</td>\n",
       "      <td>152890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-15T01:34:18Z</td>\n",
       "      <td>PART 1 - FALLS RISK STATUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>30965.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>4121621</td>\n",
       "      <td>2821</td>\n",
       "      <td>Shuvekshya</td>\n",
       "      <td>Phuyal</td>\n",
       "      <td>Registered Nurse</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-07-15T01:34:18.11Z</td>\n",
       "      <td>152890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-15T01:34:18Z</td>\n",
       "      <td>Risk Factor: recent falls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125</td>\n",
       "      <td>30968.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>4121622</td>\n",
       "      <td>2821</td>\n",
       "      <td>Shuvekshya</td>\n",
       "      <td>Phuyal</td>\n",
       "      <td>Registered Nurse</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-07-15T01:34:18.11Z</td>\n",
       "      <td>152890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-15T01:34:18Z</td>\n",
       "      <td>Risk factor: medications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>30972.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>4121623</td>\n",
       "      <td>2821</td>\n",
       "      <td>Shuvekshya</td>\n",
       "      <td>Phuyal</td>\n",
       "      <td>Registered Nurse</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-07-15T01:34:18.11Z</td>\n",
       "      <td>152890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-15T01:34:18Z</td>\n",
       "      <td>Risk factor: psychological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>127</td>\n",
       "      <td>30976.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>4121624</td>\n",
       "      <td>2821</td>\n",
       "      <td>Shuvekshya</td>\n",
       "      <td>Phuyal</td>\n",
       "      <td>Registered Nurse</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-07-15T01:34:18.11Z</td>\n",
       "      <td>152890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-15T01:34:18Z</td>\n",
       "      <td>Risk factor: cognitive status</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   resp_questionId  resp_responseId resp_dateValue resp_stringValue  \\\n",
       "0              123              NaN           None             None   \n",
       "1              124          30965.0           None             None   \n",
       "2              125          30968.0           None             None   \n",
       "3              126          30972.0           None             None   \n",
       "4              127          30976.0           None             None   \n",
       "\n",
       "   resp_numericValue resp_booleanValue resp_score  resp_recordInformation.id  \\\n",
       "0                NaN              True       None                    4121620   \n",
       "1                NaN              None          8                    4121621   \n",
       "2                NaN              None          3                    4121622   \n",
       "3                NaN              None          3                    4121623   \n",
       "4                NaN              None          3                    4121624   \n",
       "\n",
       "   resp_recordInformation.userId resp_recordInformation.userNameFirst  \\\n",
       "0                           2821                           Shuvekshya   \n",
       "1                           2821                           Shuvekshya   \n",
       "2                           2821                           Shuvekshya   \n",
       "3                           2821                           Shuvekshya   \n",
       "4                           2821                           Shuvekshya   \n",
       "\n",
       "  resp_recordInformation.userNameLast  \\\n",
       "0                              Phuyal   \n",
       "1                              Phuyal   \n",
       "2                              Phuyal   \n",
       "3                              Phuyal   \n",
       "4                              Phuyal   \n",
       "\n",
       "  resp_recordInformation.userQualification  \\\n",
       "0                         Registered Nurse   \n",
       "1                         Registered Nurse   \n",
       "2                         Registered Nurse   \n",
       "3                         Registered Nurse   \n",
       "4                         Registered Nurse   \n",
       "\n",
       "  resp_recordInformation.agencyStaffName  \\\n",
       "0                                   None   \n",
       "1                                   None   \n",
       "2                                   None   \n",
       "3                                   None   \n",
       "4                                   None   \n",
       "\n",
       "  resp_recordInformation.agencyStaffDesignation  \\\n",
       "0                                          None   \n",
       "1                                          None   \n",
       "2                                          None   \n",
       "3                                          None   \n",
       "4                                          None   \n",
       "\n",
       "  resp_recordInformation.dateCreated meta_formId meta_formVersion meta_status  \\\n",
       "0            2025-07-15T01:34:18.11Z      152890                1           1   \n",
       "1            2025-07-15T01:34:18.11Z      152890                1           1   \n",
       "2            2025-07-15T01:34:18.11Z      152890                1           1   \n",
       "3            2025-07-15T01:34:18.11Z      152890                1           1   \n",
       "4            2025-07-15T01:34:18.11Z      152890                1           1   \n",
       "\n",
       "     meta_dateCompleted meta_items.templateQuestion.text  \n",
       "0  2025-07-15T01:34:18Z       PART 1 - FALLS RISK STATUS  \n",
       "1  2025-07-15T01:34:18Z        Risk Factor: recent falls  \n",
       "2  2025-07-15T01:34:18Z         Risk factor: medications  \n",
       "3  2025-07-15T01:34:18Z       Risk factor: psychological  \n",
       "4  2025-07-15T01:34:18Z    Risk factor: cognitive status  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CM API",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "rockpool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
